{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc527f2-6cd6-43d5-b4ad-464c937ca73c",
   "metadata": {},
   "source": [
    "# Introduction.\n",
    "\n",
    "Bojanapally Santhoshini - U88362375.\n",
    "\n",
    "In this Notebook, by using sparkML we are going to solve a regression problem where we use a Linear regression model to predict a continuous variable and use root mean square error as our metric to evaluate the model, as usual first step is importing the required packages. Then we proceed to loading the data, pre-processing it then using vector assembler convert it into a linear combination of the features, to feed that to our linear regression model then evaluate the model based on Root mean square error(rmse). Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01a90449-760b-4b1e-8a5b-bea1d31a5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import findspark\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b2003d-a9fd-4837-8e75-45aec0a2353e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/29 17:52:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/29 17:52:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/04/29 17:52:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/04/29 17:52:12 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/04/29 17:52:12 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session WebUI Port: 4044\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"ISM6562 Spark App01\").enableHiveSupport().getOrCreate();\n",
    "\n",
    "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
    "sc = spark.sparkContext  \n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e818402-88fc-4366-b905-becc200ee12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.24</td>\n",
       "      <td>4.90</td>\n",
       "      <td>14.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.12</td>\n",
       "      <td>2.39</td>\n",
       "      <td>7.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.28</td>\n",
       "      <td>-4.48</td>\n",
       "      <td>-13.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-10.73</td>\n",
       "      <td>-6.11</td>\n",
       "      <td>-18.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.65</td>\n",
       "      <td>-20.30</td>\n",
       "      <td>-60.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x1     x2  target\n",
       "0  16.24   4.90   14.70\n",
       "1  -6.12   2.39    7.17\n",
       "2  -5.28  -4.48  -13.44\n",
       "3 -10.73  -6.11  -18.33\n",
       "4   8.65 -20.30  -60.90"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch dataset \n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8929a6c3-b267-409c-89f8-331fd4e6def0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1        0\n",
       "x2        0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ec210a4-97a4-4199-aab9-a8266639aad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/29 17:52:24 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b8cbe0-bcbb-4ea6-925c-a4f340feb52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------------+\n",
      "|    x1|    x2|             target|\n",
      "+------+------+-------------------+\n",
      "| 16.24|   4.9|               14.7|\n",
      "| -6.12|  2.39|               7.17|\n",
      "| -5.28| -4.48|             -13.44|\n",
      "|-10.73| -6.11|             -18.33|\n",
      "|  8.65| -20.3|-60.900000000000006|\n",
      "|-23.02|  6.08|              18.24|\n",
      "| 17.45| -3.54|             -10.62|\n",
      "| -7.61|  1.53|               4.59|\n",
      "|  3.19|  5.01|              15.03|\n",
      "| -2.49| -7.86|             -23.58|\n",
      "| 14.62| 10.17|              30.51|\n",
      "| -20.6|  1.13|               3.39|\n",
      "| -3.22| 14.97|              44.91|\n",
      "| -3.84|  1.69|               5.07|\n",
      "| 11.34|  3.19|               9.57|\n",
      "| -11.0| -2.73|              -8.19|\n",
      "| -1.72| 14.76|              44.28|\n",
      "| -8.78|-21.03|             -63.09|\n",
      "|  0.42| -5.33|             -15.99|\n",
      "|  5.83| -3.05|              -9.15|\n",
      "+------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843dbf26-56a2-46fd-84f6-49f566028b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x1: double (nullable = true)\n",
      " |-- x2: double (nullable = true)\n",
      " |-- target: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb70ac89-d87a-4ffb-aa8c-00edeb5c34da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f28261-746d-45fc-81b7-c1995104266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are only 2000 observations we are doing 80-20 split (maximum for training the model)\n",
    "\n",
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df4d9f6-96dd-4622-a9e8-b97575f91b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/29 17:54:37 WARN Instrumentation: [9b753390] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/04/29 17:54:38 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3047161790356743e-14\n",
      "Coefficients: [5.056398194956959e-16,29.90742660673947]\n",
      "Intercept: -3.634035041144678e-16\n"
     ]
    }
   ],
   "source": [
    "# Define numeric columns\n",
    "numeric_columns = ['x1', 'x2']\n",
    "\n",
    "# Define VectorAssembler for numeric features\n",
    "numeric_assembler = VectorAssembler(inputCols=numeric_columns, outputCol='numeric_feature')\n",
    "\n",
    "# Define StandardScaler for scaling numeric features\n",
    "scaler = StandardScaler(inputCol='numeric_feature', outputCol='scaled_numeric_feature')\n",
    "\n",
    "# Define VectorAssembler for features\n",
    "assembler = VectorAssembler(inputCols=['scaled_numeric_feature'], outputCol='features')\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(stages=[numeric_assembler, scaler, assembler])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "\n",
    "# Transform the training and testing data\n",
    "train_data = pipeline_model.transform(train_data)\n",
    "test_data = pipeline_model.transform(test_data)\n",
    "\n",
    "# Define the Linear Regression model\n",
    "lr_model = LinearRegression(labelCol='target')\n",
    "\n",
    "# Fit the model to the training data\n",
    "fit_model = lr_model.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = fit_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = RegressionEvaluator(labelCol='target', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Optionally, you can also print the coefficients and intercept\n",
    "coefficients = fit_model.coefficients\n",
    "intercept = fit_model.intercept\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955cbfbf-3463-4d5f-b19b-b2d66ced8e9d",
   "metadata": {},
   "source": [
    "## Conclusion.\n",
    "\n",
    "Here we can see that the root mean square value is pretty much close to zero, and also the coefficient of x1 seems to be very low which suggests there isn't much influence of x1 on our target variable and the coefficient of x2 is high approximately 30 which suggests a significant influence on our target variable. Also, the intercept is almost zero which means may be the graph passes pretty close to the origin. Since the rmse value is close to zero, it suggests either overfitting or this data is synthesized (artificial) for evaluation of regression models, in any case we can reduce this by setting a regularization parameter to non-zero, which will introduce some generalization and reduce overfitting and we can consider other evaluation metrics like Mean absolute error etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba0d1c33-96a8-47d1-81f9-5ec30a804e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without scaling (optional)\n",
    "\n",
    "# Define numeric columns\n",
    "# numeric_columns = ['x1', 'x2']\n",
    "\n",
    "# Define VectorAssembler for numeric features\n",
    "#numeric_assembler = VectorAssembler(inputCols=numeric_columns, outputCol='features')\n",
    "\n",
    "# Define the pipeline\n",
    "# pipeline = Pipeline(stages=[numeric_assembler])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "# pipeline_model = pipeline.fit(train_data)\n",
    "\n",
    "# Transform the training and testing data\n",
    "# train_data = pipeline_model.transform(train_data)\n",
    "#test_data = pipeline_model.transform(test_data)\n",
    "\n",
    "# Define the Linear Regression model\n",
    "#lr_model = LinearRegression(labelCol='target')\n",
    "\n",
    "# Fit the model to the training data\n",
    "#fit_model = lr_model.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "#predictions = fit_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "#evaluator = RegressionEvaluator(labelCol='target', predictionCol='prediction', metricName='rmse')\n",
    "#rmse = evaluator.evaluate(predictions)\n",
    "#print(\"RMSE:\", rmse)\n",
    "\n",
    "# Optionally, you can also print the coefficients and intercept\n",
    "#coefficients = fit_model.coefficients\n",
    "#intercept = fit_model.intercept\n",
    "#print(\"Coefficients:\", coefficients)\n",
    "#print(\"Intercept:\", intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a2319-1859-4cbe-9867-e320b421e909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
